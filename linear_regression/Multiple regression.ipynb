{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "This is heart data from the UMass Statistical Data website (http://www.umass.edu/statdata/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 17L)\n",
      "(200L, 1L)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.genfromtxt('heart.csv', delimiter=\",\")\n",
    "\n",
    "x = dataset[:,1:]\n",
    "x = np.insert(x,0,1,axis=1)  # Add 1's for bias\n",
    "\n",
    "print x.shape\n",
    "\n",
    "y = dataset[:,0]\n",
    "y = np.reshape(y, (y.shape[0],1))  # Reshape to a column vector\n",
    "\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 1 predictor case...\n",
    "\n",
    "The cost function for linear regression is\n",
    "1/2m(sum((theta0 + theta1(x) - y) ^ 2))\n",
    "\n",
    "Partial derivative wrt theta0: 1/m(sum(theta0 + theta1 - y))\n",
    "\n",
    "Partial derivative wrt theta1: 1/m(sum(theta0 + theta1 - y)) * x\n",
    "\n",
    "Parameter update: theta = theta - alpha(partial derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate hypothesis\n",
    "\n",
    "All vectors are treated as column vectors. Here, X is a 200x18 feature matrix, and the weights are set as 18x1 column vector\n",
    "\n",
    "Value of the hypothesis is just the dot product of X and the weights vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1379.]\n",
      " [ 1409.]\n",
      " [ 1676.]\n",
      " [ 1451.]\n",
      " [ 1297.]\n",
      " [ 1276.]\n",
      " [ 1422.]\n",
      " [ 1546.]\n",
      " [ 1638.]\n",
      " [ 1256.]\n",
      " [ 1454.]\n",
      " [ 1353.]\n",
      " [ 1283.]\n",
      " [ 1423.]\n",
      " [ 1426.]\n",
      " [ 1574.]\n",
      " [ 1441.]\n",
      " [ 1534.]\n",
      " [ 1448.]\n",
      " [ 1524.]\n",
      " [ 1417.]\n",
      " [ 1632.]\n",
      " [ 1610.]\n",
      " [ 1262.]\n",
      " [ 1468.]\n",
      " [ 1376.]\n",
      " [ 1428.]\n",
      " [ 1322.]\n",
      " [ 1506.]\n",
      " [ 1518.]\n",
      " [ 1360.]\n",
      " [ 1303.]\n",
      " [ 1219.]\n",
      " [ 1482.]\n",
      " [ 1481.]\n",
      " [ 1289.]\n",
      " [ 1477.]\n",
      " [ 1405.]\n",
      " [ 1593.]\n",
      " [ 1336.]\n",
      " [ 1495.]\n",
      " [ 1285.]\n",
      " [ 1415.]\n",
      " [ 1683.]\n",
      " [ 1330.]\n",
      " [ 1430.]\n",
      " [ 1232.]\n",
      " [ 1292.]\n",
      " [ 1534.]\n",
      " [ 1413.]\n",
      " [ 1495.]\n",
      " [ 1199.]\n",
      " [ 1514.]\n",
      " [ 1517.]\n",
      " [ 1408.]\n",
      " [ 1492.]\n",
      " [ 1257.]\n",
      " [ 1357.]\n",
      " [ 1384.]\n",
      " [ 1659.]\n",
      " [ 1445.]\n",
      " [ 1296.]\n",
      " [ 1428.]\n",
      " [ 1316.]\n",
      " [ 1599.]\n",
      " [ 1573.]\n",
      " [ 1468.]\n",
      " [ 1552.]\n",
      " [ 1404.]\n",
      " [ 1368.]\n",
      " [ 1396.]\n",
      " [ 1460.]\n",
      " [ 1242.]\n",
      " [ 1435.]\n",
      " [ 1325.]\n",
      " [ 1439.]\n",
      " [ 1707.]\n",
      " [ 1371.]\n",
      " [ 1560.]\n",
      " [ 1326.]\n",
      " [ 1720.]\n",
      " [ 1274.]\n",
      " [ 1319.]\n",
      " [ 1517.]\n",
      " [ 1324.]\n",
      " [ 1333.]\n",
      " [ 1369.]\n",
      " [ 1430.]\n",
      " [ 1418.]\n",
      " [ 1508.]\n",
      " [ 1189.]\n",
      " [ 1356.]\n",
      " [ 1475.]\n",
      " [ 1329.]\n",
      " [ 1484.]\n",
      " [ 1345.]\n",
      " [ 1264.]\n",
      " [ 1228.]\n",
      " [ 1452.]\n",
      " [ 1296.]\n",
      " [ 1387.]\n",
      " [ 1396.]\n",
      " [ 1302.]\n",
      " [ 1464.]\n",
      " [ 1505.]\n",
      " [ 1505.]\n",
      " [ 1495.]\n",
      " [ 1395.]\n",
      " [ 1468.]\n",
      " [ 1380.]\n",
      " [ 1303.]\n",
      " [ 1506.]\n",
      " [ 1375.]\n",
      " [ 1321.]\n",
      " [ 1245.]\n",
      " [ 1337.]\n",
      " [ 1493.]\n",
      " [ 1411.]\n",
      " [ 1468.]\n",
      " [ 1428.]\n",
      " [ 1281.]\n",
      " [ 1271.]\n",
      " [ 1482.]\n",
      " [ 1368.]\n",
      " [ 1422.]\n",
      " [ 1478.]\n",
      " [ 1244.]\n",
      " [ 1372.]\n",
      " [ 1403.]\n",
      " [ 1240.]\n",
      " [ 1573.]\n",
      " [ 1448.]\n",
      " [ 1406.]\n",
      " [ 1514.]\n",
      " [ 1423.]\n",
      " [ 1120.]\n",
      " [ 1268.]\n",
      " [ 1478.]\n",
      " [ 1301.]\n",
      " [ 1357.]\n",
      " [ 1361.]\n",
      " [ 1396.]\n",
      " [ 1291.]\n",
      " [ 1269.]\n",
      " [ 1306.]\n",
      " [ 1447.]\n",
      " [ 1331.]\n",
      " [ 1455.]\n",
      " [ 1284.]\n",
      " [ 1225.]\n",
      " [ 1516.]\n",
      " [ 1408.]\n",
      " [ 1406.]\n",
      " [ 1259.]\n",
      " [ 1496.]\n",
      " [ 1159.]\n",
      " [ 1530.]\n",
      " [ 1421.]\n",
      " [ 1345.]\n",
      " [ 1339.]\n",
      " [ 1259.]\n",
      " [ 1348.]\n",
      " [ 1263.]\n",
      " [ 1238.]\n",
      " [ 1434.]\n",
      " [ 1395.]\n",
      " [ 1604.]\n",
      " [ 1231.]\n",
      " [ 1243.]\n",
      " [ 1347.]\n",
      " [ 1306.]\n",
      " [ 1548.]\n",
      " [ 1345.]\n",
      " [ 1312.]\n",
      " [ 1417.]\n",
      " [ 1354.]\n",
      " [ 1146.]\n",
      " [ 1455.]\n",
      " [ 1413.]\n",
      " [ 1385.]\n",
      " [ 1611.]\n",
      " [ 1389.]\n",
      " [ 1371.]\n",
      " [ 1101.]\n",
      " [ 1234.]\n",
      " [ 1234.]\n",
      " [ 1310.]\n",
      " [ 1395.]\n",
      " [ 1460.]\n",
      " [ 1267.]\n",
      " [ 1339.]\n",
      " [ 1426.]\n",
      " [ 1200.]\n",
      " [ 1238.]\n",
      " [ 1207.]\n",
      " [ 1327.]\n",
      " [ 1060.]\n",
      " [ 1293.]\n",
      " [ 1127.]\n",
      " [ 1282.]]\n"
     ]
    }
   ],
   "source": [
    "def h(weights, X):\n",
    "    \"\"\" Calculate the hypothesis value.\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    \n",
    "    Return:\n",
    "    An mx1 column vector containing the hypothesis values for each training example\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Test that we correctly evaluate the hypothesis value for each training example\n",
    "theta = np.ones((x.shape[1],1))\n",
    "print h(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cost\n",
    "\n",
    "For each training example we calculate the total residual value using all available predictors.\n",
    "\n",
    "Then we square each residual/error value (and sum across all training examples) with the dot product. The average squared error is then returned as the cost/loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 908907.6225]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(weights, X, Y):\n",
    "    \"\"\"Calculate total cost across all samples and features for a given set of weights\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    Y -- a mx1 column vector containing the labels/targets for each training example\n",
    "    \n",
    "    Returns:\n",
    "    residuals -- mx1 array of residuals for each training example\n",
    "    total_cost -- 1x1 float value representing the average squared error across the entire dataset for a given set of weights\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]  # Number of training examples. Equivalent to X.shape[0]\n",
    "    residuals = h(weights, X) - Y  # mx1 column vector containing residual for each training example\n",
    "    squared_error = np.dot(residuals.T, residuals)  # 1x1 containing the total squared error\n",
    "    total_cost = float(1)/(2*m) * squared_error  # 1x1 containing average cost value over all training examples\n",
    "    \n",
    "    return residuals, total_cost\n",
    "\n",
    "# Test that we return a scalar cost value, calculated across all training examples and all features for a given set of weights\n",
    "print calculate_cost(theta, x, y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Simultaneously update the weight values after calculating the partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 | Cost: 908907.622500\n",
      "Iteration: 2 | Cost: 518739.181445\n",
      "Iteration: 3 | Cost: 296283.030309\n",
      "Iteration: 4 | Cost: 169447.872578\n",
      "Iteration: 5 | Cost: 97130.931931\n",
      "Iteration: 6 | Cost: 55897.503979\n",
      "Iteration: 7 | Cost: 32386.316818\n",
      "Iteration: 8 | Cost: 18979.455394\n",
      "Iteration: 9 | Cost: 11333.573472\n",
      "Iteration: 10 | Cost: 6972.316759\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(weights, X, Y, iterations = 1000, alpha = 1e-6, verbose = True):\n",
    "    \"\"\"Update weight values using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    Y -- a mx1 column vector containing the labels/targets for each training example\n",
    "    iterations -- number of training iterations to perform\n",
    "    alpha -- learning rate. Step size multiplier for each weight adjustment\n",
    "    verbose -- print the current iteration number and current cost value\n",
    "    \n",
    "    Returns:\n",
    "    cost_history -- numpy array containing the cost values for each iteration\n",
    "    theta -- vector of final weights after all training iterations\n",
    "    \"\"\"\n",
    "    theta = weights\n",
    "    m = Y.shape[0]  # Number of training examples. Equivalent to X.shape[0]\n",
    "    cost_history = np.zeros(iterations)  # Initialize array of cost history values with 0's\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        residuals, cost = calculate_cost(theta, X, Y)\n",
    "        gradient = (float(1)/m) * np.dot(residuals.T, X).T  #nx1 column vector containing current gradient of each variable\n",
    "        theta -= (alpha * gradient)  #nx1 column vector containing updated all updated weight values\n",
    "        \n",
    "        # Store the cost for this iteration\n",
    "        cost_history[i] = cost\n",
    "        \n",
    "        if verbose:\n",
    "            print \"Iteration: %d | Cost: %f\" % (i+1, cost)\n",
    "\n",
    "    return cost_history, theta\n",
    "\n",
    "# Test gradient descent over 10 iterations\n",
    "initial_weights = np.ones((x.shape[1],1))\n",
    "history, final_weights = gradient_descent(initial_weights, x, y, iterations = 10)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Plot training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEZCAYAAACjPJNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x98ab208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_curve(history):\n",
    "    \"\"\"Plot the training curve.\n",
    "    \n",
    "    Parameters:\n",
    "    history -- numpy array/list of cost values over all training iterations\n",
    "    \n",
    "    Returns:\n",
    "    Plot of the cost for each iteration of training\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(range(1, len(history)+1), history)\n",
    "    plt.grid(True)\n",
    "    plt.xlim(1, len(history))\n",
    "    plt.ylim(min(history), max(history))\n",
    "    \n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "\n",
    "# Test training curve plotting\n",
    "initial_weights = np.ones((x.shape[1],1))\n",
    "history, final_weights = gradient_descent(initial_weights, x, y, iterations = 10, verbose = False)\n",
    "plot_training_curve(history)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

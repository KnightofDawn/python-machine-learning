{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "This is heart data from the UMass Statistical Data website (http://www.umass.edu/statdata/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 17L)\n",
      "(200L, 1L)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.genfromtxt('heart.csv', delimiter=\",\")\n",
    "\n",
    "x = dataset[:,1:]\n",
    "x = np.insert(x,0,1,axis=1)  # Add 1's for bias\n",
    "\n",
    "print x.shape\n",
    "\n",
    "y = dataset[:,0]\n",
    "y = np.reshape(y, (y.shape[0],1))  # Reshape to a column vector\n",
    "\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 1 predictor case...\n",
    "\n",
    "The cost function for linear regression is\n",
    "1/2m(sum((theta0 + theta1(x) - y) ^ 2))\n",
    "\n",
    "Partial derivative wrt theta0: 1/m(sum(theta0 + theta1 - y))\n",
    "\n",
    "Partial derivative wrt theta1: 1/m(sum(theta0 + theta1 - y)) * x\n",
    "\n",
    "Parameter update: theta = theta - alpha(partial derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate hypothesis\n",
    "\n",
    "All vectors are treated as column vectors. Here, X is a 200x18 feature matrix, and the weights are set as 18x1 column vector\n",
    "\n",
    "Value of the hypothesis is just the dot product of X and the weights vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1379.]\n",
      " [ 1409.]\n",
      " [ 1676.]\n",
      " [ 1451.]\n",
      " [ 1297.]\n",
      " [ 1276.]\n",
      " [ 1422.]\n",
      " [ 1546.]\n",
      " [ 1638.]\n",
      " [ 1256.]\n",
      " [ 1454.]\n",
      " [ 1353.]\n",
      " [ 1283.]\n",
      " [ 1423.]\n",
      " [ 1426.]\n",
      " [ 1574.]\n",
      " [ 1441.]\n",
      " [ 1534.]\n",
      " [ 1448.]\n",
      " [ 1524.]\n",
      " [ 1417.]\n",
      " [ 1632.]\n",
      " [ 1610.]\n",
      " [ 1262.]\n",
      " [ 1468.]\n",
      " [ 1376.]\n",
      " [ 1428.]\n",
      " [ 1322.]\n",
      " [ 1506.]\n",
      " [ 1518.]\n",
      " [ 1360.]\n",
      " [ 1303.]\n",
      " [ 1219.]\n",
      " [ 1482.]\n",
      " [ 1481.]\n",
      " [ 1289.]\n",
      " [ 1477.]\n",
      " [ 1405.]\n",
      " [ 1593.]\n",
      " [ 1336.]\n",
      " [ 1495.]\n",
      " [ 1285.]\n",
      " [ 1415.]\n",
      " [ 1683.]\n",
      " [ 1330.]\n",
      " [ 1430.]\n",
      " [ 1232.]\n",
      " [ 1292.]\n",
      " [ 1534.]\n",
      " [ 1413.]\n",
      " [ 1495.]\n",
      " [ 1199.]\n",
      " [ 1514.]\n",
      " [ 1517.]\n",
      " [ 1408.]\n",
      " [ 1492.]\n",
      " [ 1257.]\n",
      " [ 1357.]\n",
      " [ 1384.]\n",
      " [ 1659.]\n",
      " [ 1445.]\n",
      " [ 1296.]\n",
      " [ 1428.]\n",
      " [ 1316.]\n",
      " [ 1599.]\n",
      " [ 1573.]\n",
      " [ 1468.]\n",
      " [ 1552.]\n",
      " [ 1404.]\n",
      " [ 1368.]\n",
      " [ 1396.]\n",
      " [ 1460.]\n",
      " [ 1242.]\n",
      " [ 1435.]\n",
      " [ 1325.]\n",
      " [ 1439.]\n",
      " [ 1707.]\n",
      " [ 1371.]\n",
      " [ 1560.]\n",
      " [ 1326.]\n",
      " [ 1720.]\n",
      " [ 1274.]\n",
      " [ 1319.]\n",
      " [ 1517.]\n",
      " [ 1324.]\n",
      " [ 1333.]\n",
      " [ 1369.]\n",
      " [ 1430.]\n",
      " [ 1418.]\n",
      " [ 1508.]\n",
      " [ 1189.]\n",
      " [ 1356.]\n",
      " [ 1475.]\n",
      " [ 1329.]\n",
      " [ 1484.]\n",
      " [ 1345.]\n",
      " [ 1264.]\n",
      " [ 1228.]\n",
      " [ 1452.]\n",
      " [ 1296.]\n",
      " [ 1387.]\n",
      " [ 1396.]\n",
      " [ 1302.]\n",
      " [ 1464.]\n",
      " [ 1505.]\n",
      " [ 1505.]\n",
      " [ 1495.]\n",
      " [ 1395.]\n",
      " [ 1468.]\n",
      " [ 1380.]\n",
      " [ 1303.]\n",
      " [ 1506.]\n",
      " [ 1375.]\n",
      " [ 1321.]\n",
      " [ 1245.]\n",
      " [ 1337.]\n",
      " [ 1493.]\n",
      " [ 1411.]\n",
      " [ 1468.]\n",
      " [ 1428.]\n",
      " [ 1281.]\n",
      " [ 1271.]\n",
      " [ 1482.]\n",
      " [ 1368.]\n",
      " [ 1422.]\n",
      " [ 1478.]\n",
      " [ 1244.]\n",
      " [ 1372.]\n",
      " [ 1403.]\n",
      " [ 1240.]\n",
      " [ 1573.]\n",
      " [ 1448.]\n",
      " [ 1406.]\n",
      " [ 1514.]\n",
      " [ 1423.]\n",
      " [ 1120.]\n",
      " [ 1268.]\n",
      " [ 1478.]\n",
      " [ 1301.]\n",
      " [ 1357.]\n",
      " [ 1361.]\n",
      " [ 1396.]\n",
      " [ 1291.]\n",
      " [ 1269.]\n",
      " [ 1306.]\n",
      " [ 1447.]\n",
      " [ 1331.]\n",
      " [ 1455.]\n",
      " [ 1284.]\n",
      " [ 1225.]\n",
      " [ 1516.]\n",
      " [ 1408.]\n",
      " [ 1406.]\n",
      " [ 1259.]\n",
      " [ 1496.]\n",
      " [ 1159.]\n",
      " [ 1530.]\n",
      " [ 1421.]\n",
      " [ 1345.]\n",
      " [ 1339.]\n",
      " [ 1259.]\n",
      " [ 1348.]\n",
      " [ 1263.]\n",
      " [ 1238.]\n",
      " [ 1434.]\n",
      " [ 1395.]\n",
      " [ 1604.]\n",
      " [ 1231.]\n",
      " [ 1243.]\n",
      " [ 1347.]\n",
      " [ 1306.]\n",
      " [ 1548.]\n",
      " [ 1345.]\n",
      " [ 1312.]\n",
      " [ 1417.]\n",
      " [ 1354.]\n",
      " [ 1146.]\n",
      " [ 1455.]\n",
      " [ 1413.]\n",
      " [ 1385.]\n",
      " [ 1611.]\n",
      " [ 1389.]\n",
      " [ 1371.]\n",
      " [ 1101.]\n",
      " [ 1234.]\n",
      " [ 1234.]\n",
      " [ 1310.]\n",
      " [ 1395.]\n",
      " [ 1460.]\n",
      " [ 1267.]\n",
      " [ 1339.]\n",
      " [ 1426.]\n",
      " [ 1200.]\n",
      " [ 1238.]\n",
      " [ 1207.]\n",
      " [ 1327.]\n",
      " [ 1060.]\n",
      " [ 1293.]\n",
      " [ 1127.]\n",
      " [ 1282.]]\n"
     ]
    }
   ],
   "source": [
    "def h(weights, X):\n",
    "    \"\"\" Calculate the hypothesis value.\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    \n",
    "    Return:\n",
    "    An mx1 column vector containing the hypothesis values for each training example\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Test that we correctly evaluate the hypothesis value for each training example\n",
    "theta = np.ones((x.shape[1],1))\n",
    "print h(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cost\n",
    "\n",
    "For each training example we calculate the total residual value using all available predictors.\n",
    "\n",
    "Then we square each residual/error value (and sum across all training examples) with the dot product. The average squared error is then returned as the cost/loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 908907.6225]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(weights, X, Y):\n",
    "    \"\"\"Calculate total cost across all samples and features for a given set of weights\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    Y -- a mx1 column vector containing the labels/targets for each training example\n",
    "    \n",
    "    Returns:\n",
    "    residuals -- mx1 array of residuals for each training example\n",
    "    total_cost -- 1x1 float value representing the average squared error across the entire dataset for a given set of weights\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]  # Number of training examples. Equivalent to X.shape[0]\n",
    "    residuals = h(weights, X) - Y  # mx1 column vector containing residual for each training example\n",
    "    squared_error = np.dot(residuals.T, residuals)  # 1x1 containing the total squared error\n",
    "    total_cost = float(1)/(2*m) * squared_error  # 1x1 containing average cost value over all training examples\n",
    "    \n",
    "    return residuals, total_cost\n",
    "\n",
    "# Test that we return a scalar cost value, calculated across all training examples and all features for a given set of weights\n",
    "print calculate_cost(theta, x, y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Simultaneously update the weight values after calculating the partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 | Cost: 908907.622500\n",
      "Iteration: 2 | Cost: 518739.181445\n",
      "Iteration: 3 | Cost: 296283.030309\n",
      "Iteration: 4 | Cost: 169447.872578\n",
      "Iteration: 5 | Cost: 97130.931931\n",
      "Iteration: 6 | Cost: 55897.503979\n",
      "Iteration: 7 | Cost: 32386.316818\n",
      "Iteration: 8 | Cost: 18979.455394\n",
      "Iteration: 9 | Cost: 11333.573472\n",
      "Iteration: 10 | Cost: 6972.316759\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(weights, X, Y, iterations = 1000, alpha = 1e-6, verbose = True):\n",
    "    \"\"\"Update weight values using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    Y -- a mx1 column vector containing the labels/targets for each training example\n",
    "    iterations -- number of training iterations to perform\n",
    "    alpha -- learning rate. Step size multiplier for each weight adjustment\n",
    "    verbose -- print the current iteration number and current cost value\n",
    "    \n",
    "    Returns:\n",
    "    cost_history -- numpy array containing the cost values for each iteration\n",
    "    theta -- vector of final weights after all training iterations\n",
    "    \"\"\"\n",
    "    theta = weights\n",
    "    m = Y.shape[0]  # Number of training examples. Equivalent to X.shape[0]\n",
    "    cost_history = np.zeros(iterations)  # Initialize array of cost history values with 0's\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        residuals, cost = calculate_cost(theta, X, Y)\n",
    "        gradient = (float(1)/m) * np.dot(residuals.T, X).T  #nx1 column vector containing current gradient of each variable\n",
    "        theta -= (alpha * gradient)  #nx1 column vector containing updated all updated weight values\n",
    "        \n",
    "        # Store the cost for this iteration\n",
    "        cost_history[i] = cost\n",
    "        \n",
    "        if verbose:\n",
    "            print \"Iteration: %d | Cost: %f\" % (i+1, cost)\n",
    "\n",
    "    return cost_history, theta\n",
    "\n",
    "# Test gradient descent over 10 iterations\n",
    "initial_weights = np.ones((x.shape[1],1))\n",
    "history, final_weights = gradient_descent(initial_weights, x, y, iterations = 10)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Plot training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99321344]\n",
      " [ 0.98434976]\n",
      " [ 0.02111763]\n",
      " [ 0.35967361]\n",
      " [ 0.53261426]\n",
      " [-0.1248873 ]\n",
      " [-0.01577066]\n",
      " [ 0.97827294]\n",
      " [ 0.9522199 ]\n",
      " [ 0.98082257]\n",
      " [-0.05395413]\n",
      " [ 0.35041543]\n",
      " [-0.10671318]\n",
      " [-0.10562628]\n",
      " [ 0.99394339]\n",
      " [ 0.38261906]\n",
      " [ 0.99054531]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEZCAYAAAA6xErnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZGZmxbnQfctvAAAAC0lEQVQYmZlZcf8fh60iUEhhgwIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x781b780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_curve(history):\n",
    "    \"\"\"Plot the training curve.\n",
    "    \n",
    "    Parameters:\n",
    "    history -- numpy array/list of cost values over all training iterations\n",
    "    \n",
    "    Returns:\n",
    "    Plot of the cost for each iteration of training\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(range(1, len(history)+1), history)\n",
    "    plt.grid(True)\n",
    "    plt.xlim(1, len(history))\n",
    "    plt.ylim(min(history), max(history))\n",
    "    \n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "\n",
    "# Test training curve plotting\n",
    "initial_weights = np.ones((x.shape[1],1))\n",
    "history, final_weights = gradient_descent(initial_weights, x, y, iterations = 1000, verbose = False)\n",
    "plot_training_curve(history)\n",
    "print final_weights"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "This is heart data from the UMass Statistical Data website (http://www.umass.edu/statdata/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 18L)\n",
      "(200L, 1L)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.genfromtxt('heart.csv', delimiter=\",\")\n",
    "\n",
    "x = dataset[:]  # Age\n",
    "x = np.insert(x,0,1,axis=1)  # Add 1's for bias\n",
    "\n",
    "print x.shape\n",
    "\n",
    "y = dataset[:,6]  # Cholesterol\n",
    "y = np.reshape(y, (y.shape[0],1))  # Reshape to a column vector\n",
    "\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 1 predictor case...\n",
    "\n",
    "The cost function for linear regression is\n",
    "1/2m(sum((theta0 + theta1(x) - y) ^ 2))\n",
    "\n",
    "Partial derivative wrt theta0: 1/m(sum(theta0 + theta1 - y))\n",
    "\n",
    "Partial derivative wrt theta1: 1/m(sum(theta0 + theta1 - y)) * x\n",
    "\n",
    "Parameter update: theta = theta - alpha(partial derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate hypothesis\n",
    "\n",
    "All vectors are treated as column vectors. Here, X is a 200x18 feature matrix, and the weights are set as 18x1 column vector\n",
    "\n",
    "Value of the hypothesis is just the dot product of X and the weights vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1421.]\n",
      " [ 1462.]\n",
      " [ 1729.]\n",
      " [ 1499.]\n",
      " [ 1350.]\n",
      " [ 1334.]\n",
      " [ 1470.]\n",
      " [ 1606.]\n",
      " [ 1697.]\n",
      " [ 1296.]\n",
      " [ 1510.]\n",
      " [ 1411.]\n",
      " [ 1347.]\n",
      " [ 1480.]\n",
      " [ 1458.]\n",
      " [ 1633.]\n",
      " [ 1489.]\n",
      " [ 1581.]\n",
      " [ 1495.]\n",
      " [ 1552.]\n",
      " [ 1454.]\n",
      " [ 1686.]\n",
      " [ 1648.]\n",
      " [ 1314.]\n",
      " [ 1514.]\n",
      " [ 1427.]\n",
      " [ 1477.]\n",
      " [ 1368.]\n",
      " [ 1532.]\n",
      " [ 1553.]\n",
      " [ 1405.]\n",
      " [ 1360.]\n",
      " [ 1243.]\n",
      " [ 1546.]\n",
      " [ 1515.]\n",
      " [ 1319.]\n",
      " [ 1529.]\n",
      " [ 1461.]\n",
      " [ 1637.]\n",
      " [ 1381.]\n",
      " [ 1524.]\n",
      " [ 1327.]\n",
      " [ 1461.]\n",
      " [ 1728.]\n",
      " [ 1381.]\n",
      " [ 1474.]\n",
      " [ 1264.]\n",
      " [ 1326.]\n",
      " [ 1564.]\n",
      " [ 1453.]\n",
      " [ 1529.]\n",
      " [ 1242.]\n",
      " [ 1553.]\n",
      " [ 1562.]\n",
      " [ 1449.]\n",
      " [ 1549.]\n",
      " [ 1310.]\n",
      " [ 1412.]\n",
      " [ 1441.]\n",
      " [ 1697.]\n",
      " [ 1490.]\n",
      " [ 1329.]\n",
      " [ 1489.]\n",
      " [ 1352.]\n",
      " [ 1636.]\n",
      " [ 1624.]\n",
      " [ 1514.]\n",
      " [ 1603.]\n",
      " [ 1464.]\n",
      " [ 1425.]\n",
      " [ 1459.]\n",
      " [ 1523.]\n",
      " [ 1310.]\n",
      " [ 1492.]\n",
      " [ 1389.]\n",
      " [ 1502.]\n",
      " [ 1769.]\n",
      " [ 1426.]\n",
      " [ 1610.]\n",
      " [ 1365.]\n",
      " [ 1770.]\n",
      " [ 1316.]\n",
      " [ 1372.]\n",
      " [ 1573.]\n",
      " [ 1386.]\n",
      " [ 1394.]\n",
      " [ 1426.]\n",
      " [ 1473.]\n",
      " [ 1483.]\n",
      " [ 1557.]\n",
      " [ 1236.]\n",
      " [ 1415.]\n",
      " [ 1528.]\n",
      " [ 1372.]\n",
      " [ 1536.]\n",
      " [ 1410.]\n",
      " [ 1314.]\n",
      " [ 1261.]\n",
      " [ 1501.]\n",
      " [ 1358.]\n",
      " [ 1432.]\n",
      " [ 1449.]\n",
      " [ 1332.]\n",
      " [ 1489.]\n",
      " [ 1531.]\n",
      " [ 1574.]\n",
      " [ 1555.]\n",
      " [ 1440.]\n",
      " [ 1523.]\n",
      " [ 1443.]\n",
      " [ 1355.]\n",
      " [ 1559.]\n",
      " [ 1430.]\n",
      " [ 1369.]\n",
      " [ 1295.]\n",
      " [ 1381.]\n",
      " [ 1542.]\n",
      " [ 1472.]\n",
      " [ 1510.]\n",
      " [ 1475.]\n",
      " [ 1321.]\n",
      " [ 1304.]\n",
      " [ 1511.]\n",
      " [ 1411.]\n",
      " [ 1473.]\n",
      " [ 1535.]\n",
      " [ 1274.]\n",
      " [ 1416.]\n",
      " [ 1447.]\n",
      " [ 1278.]\n",
      " [ 1608.]\n",
      " [ 1482.]\n",
      " [ 1436.]\n",
      " [ 1565.]\n",
      " [ 1474.]\n",
      " [ 1177.]\n",
      " [ 1314.]\n",
      " [ 1528.]\n",
      " [ 1326.]\n",
      " [ 1412.]\n",
      " [ 1408.]\n",
      " [ 1445.]\n",
      " [ 1338.]\n",
      " [ 1303.]\n",
      " [ 1353.]\n",
      " [ 1493.]\n",
      " [ 1376.]\n",
      " [ 1492.]\n",
      " [ 1320.]\n",
      " [ 1267.]\n",
      " [ 1573.]\n",
      " [ 1451.]\n",
      " [ 1464.]\n",
      " [ 1293.]\n",
      " [ 1525.]\n",
      " [ 1202.]\n",
      " [ 1578.]\n",
      " [ 1462.]\n",
      " [ 1399.]\n",
      " [ 1386.]\n",
      " [ 1294.]\n",
      " [ 1396.]\n",
      " [ 1312.]\n",
      " [ 1263.]\n",
      " [ 1482.]\n",
      " [ 1437.]\n",
      " [ 1638.]\n",
      " [ 1270.]\n",
      " [ 1289.]\n",
      " [ 1377.]\n",
      " [ 1370.]\n",
      " [ 1595.]\n",
      " [ 1381.]\n",
      " [ 1342.]\n",
      " [ 1465.]\n",
      " [ 1388.]\n",
      " [ 1172.]\n",
      " [ 1477.]\n",
      " [ 1439.]\n",
      " [ 1439.]\n",
      " [ 1664.]\n",
      " [ 1437.]\n",
      " [ 1413.]\n",
      " [ 1143.]\n",
      " [ 1294.]\n",
      " [ 1269.]\n",
      " [ 1362.]\n",
      " [ 1439.]\n",
      " [ 1501.]\n",
      " [ 1295.]\n",
      " [ 1375.]\n",
      " [ 1480.]\n",
      " [ 1249.]\n",
      " [ 1287.]\n",
      " [ 1268.]\n",
      " [ 1377.]\n",
      " [ 1083.]\n",
      " [ 1313.]\n",
      " [ 1173.]\n",
      " [ 1318.]]\n"
     ]
    }
   ],
   "source": [
    "def h(weights, X):\n",
    "    \"\"\" Calculate the hypothesis value.\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    \n",
    "    Return:\n",
    "    A mx1 column vector containing the hypothesis values for each training example\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Test that we correctly evaluate the hypothesis value for each training example\n",
    "theta = np.ones((x.shape[1],1))\n",
    "print h(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cost\n",
    "\n",
    "For each training example we calculate the total residual value using all available predictors.\n",
    "\n",
    "Then we square each residual/error value (and sum across all training examples) with the dot product. The average squared error is then returned as the cost/loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 697361.66]]\n"
     ]
    }
   ],
   "source": [
    "def calculateCost(weights, X, Y):\n",
    "    \"\"\"Calculate total cost across all samples and features for a given set of weights\n",
    "    \n",
    "    Parameters:\n",
    "    weights -- a nx1 column vector, where n is the number of features/variables plus 1 for the bias/intercept\n",
    "    X -- a mxn matrix, where m is the number of training examples, plus a column of 1's for the bias\n",
    "    Y -- a mx1 column vector containing the labels/targets for each training example\n",
    "    \n",
    "    Returns:\n",
    "    Float value representing the average squared error across the entire dataset for a given set of weights\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]  # Number of training examples. Equivalent to X.shape[0]\n",
    "    residuals = h(weights, X) - Y\n",
    "    squared_error = np.dot(residuals.T, residuals)\n",
    "    \n",
    "    return residuals, float(1)/(2*m) * squared_error\n",
    "\n",
    "# Test that we return a scalar cost value, calculated across all training examples and all features for a given set of weights\n",
    "print calculateCost(theta, x, y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Simultaneously update the weight values after calculating the partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 | Cost: 697361.660000\n",
      "Iteration: 2 | Cost: 42325117406694536.000000\n",
      "Iteration: 3 | Cost: 2582619233752172973298548736.000000\n",
      "Iteration: 4 | Cost: 157587870187822131053636619678439702528.000000\n",
      "Iteration: 5 | Cost: 9615794890267613993157742129590663647488278265856.000000\n",
      "Iteration: 6 | Cost: 586742566299639056214850053126373470375291925502292628865024.000000\n",
      "Iteration: 7 | Cost: 35802223636896341252024520466351286036210695981332341244471123386564608.000000\n",
      "Iteration: 8 | Cost: 2184602397999103721235495170337773586803587670311485720191999268103203305868492800.000000\n",
      "Iteration: 9 | Cost: 133301430820210137306973920572060622124960543070631455867062328185612830875917185200255139840.000000\n",
      "Iteration: 10 | Cost: 8133869794792090420197211901256662534414804874768261228682962422940850131232543285618028118874154598400.000000\n"
     ]
    }
   ],
   "source": [
    "def gradientDescent(weights, X, Y, iterations = 1000, alpha = 0.01):\n",
    "    \"\"\"Update weight values using gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Partial derivative wrt theta1: 1/m(sum(theta0 + theta1 - y)) * x\n",
    "    \"\"\"\n",
    "    theta = weights\n",
    "    m = Y.shape[0]\n",
    "    cost_history = []\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        residuals, cost = calculateCost(theta, X, Y)\n",
    "        gradient = (float(1)/m) * np.dot(residuals.T, X).T\n",
    "        theta =- (alpha * gradient)\n",
    "        \n",
    "        # Store the cost for this iteration\n",
    "        cost_history.append(cost)\n",
    "        print \"Iteration: %d | Cost: %f\" % (i+1, cost)\n",
    "\n",
    "gradientDescent(np.ones((x.shape[1],1)), x, y, 10, 1)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
